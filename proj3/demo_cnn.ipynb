{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@Description :   CNN 方法对比\n",
    "@Author      :   Xubo Luo \n",
    "@Time        :   2024/01/08 16:04:58\n",
    "\"\"\"\n",
    "\n",
    "from utils.core import *\n",
    "from utils.torch_backend import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络&函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ColorMap()\n",
    "draw = lambda graph: display(DotGraph({p: ({'fillcolor': colors[type(v)], 'tooltip': repr(v)}, inputs) for p, (v, inputs) in graph.items() if v is not None}))\n",
    "\n",
    "\n",
    "batch_norm = partial(BatchNorm, weight_init=None, bias_init=None)\n",
    "\n",
    "def res_block(c_in, c_out, stride, **kw):\n",
    "    block = {\n",
    "        'bn1': batch_norm(c_in, **kw),\n",
    "        'relu1': nn.ReLU(True),\n",
    "        'branch': {\n",
    "            'conv1': nn.Conv2d(c_in, c_out, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            'bn2': batch_norm(c_out, **kw),\n",
    "            'relu2': nn.ReLU(True),\n",
    "            'conv2': nn.Conv2d(c_out, c_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "        }\n",
    "    }\n",
    "    projection = (stride != 1) or (c_in != c_out)    \n",
    "    if projection:\n",
    "        block['conv3'] = (nn.Conv2d(c_in, c_out, kernel_size=1, stride=stride, padding=0, bias=False), ['relu1'])\n",
    "    block['add'] =  (Add(), [('conv3' if projection else 'relu1'), 'branch/conv2'])\n",
    "    return block\n",
    "\n",
    "def DAWN_net(c=64, block=res_block, prep_bn_relu=False, concat_pool=True, **kw):    \n",
    "    if isinstance(c, int):\n",
    "        c = [c, 2*c, 4*c, 4*c]\n",
    "        \n",
    "    classifier_pool = {\n",
    "        'in': Identity(),\n",
    "        'maxpool': nn.MaxPool2d(4),\n",
    "        'avgpool': (nn.AvgPool2d(4), ['in']),\n",
    "        'concat': (Concat(), ['maxpool', 'avgpool']),\n",
    "    } if concat_pool else {'pool': nn.MaxPool2d(4)}\n",
    "    \n",
    "    return {\n",
    "        'input': (None, []),\n",
    "        'prep': union({'conv': nn.Conv2d(3, c[0], kernel_size=3, stride=1, padding=1, bias=False)},\n",
    "                      {'bn': batch_norm(c[0], **kw), 'relu': nn.ReLU(True)} if prep_bn_relu else {}),\n",
    "        'layer1': {\n",
    "            'block0': block(c[0], c[0], 1, **kw),\n",
    "            'block1': block(c[0], c[0], 1, **kw),\n",
    "        },\n",
    "        'layer2': {\n",
    "            'block0': block(c[0], c[1], 2, **kw),\n",
    "            'block1': block(c[1], c[1], 1, **kw),\n",
    "        },\n",
    "        'layer3': {\n",
    "            'block0': block(c[1], c[2], 2, **kw),\n",
    "            'block1': block(c[2], c[2], 1, **kw),\n",
    "        },\n",
    "        'layer4': {\n",
    "            'block0': block(c[2], c[3], 2, **kw),\n",
    "            'block1': block(c[3], c[3], 1, **kw),\n",
    "        },\n",
    "        'final': union(classifier_pool, {\n",
    "            'flatten': Flatten(),\n",
    "            'linear': nn.Linear(2*c[3] if concat_pool else c[3], 10, bias=True),\n",
    "        }),\n",
    "        'logits': Identity(),\n",
    "    }\n",
    "\n",
    "\n",
    "def conv_bn(c_in, c_out, bn_weight_init=1.0, **kw):\n",
    "    return {\n",
    "        'conv': nn.Conv2d(c_in, c_out, kernel_size=3, stride=1, padding=1, bias=False), \n",
    "        'bn': batch_norm(c_out, **kw), \n",
    "        # 'bn': batch_norm(c_out, bn_weight_init=bn_weight_init, **kw), \n",
    "        'relu': nn.ReLU(True)\n",
    "    }\n",
    "\n",
    "def basic_net(channels, weight,  pool, **kw):\n",
    "    return {\n",
    "        'input': (None, []),\n",
    "        'prep': conv_bn(3, channels['prep'], **kw),\n",
    "        'layer1': dict(conv_bn(channels['prep'], channels['layer1'], **kw), pool=pool),\n",
    "        'layer2': dict(conv_bn(channels['layer1'], channels['layer2'], **kw), pool=pool),\n",
    "        'layer3': dict(conv_bn(channels['layer2'], channels['layer3'], **kw), pool=pool),\n",
    "        'pool': nn.MaxPool2d(4),\n",
    "        'flatten': Flatten(),\n",
    "        'linear': nn.Linear(channels['layer3'], 10, bias=False),\n",
    "        'logits': Mul(weight),\n",
    "    }\n",
    "\n",
    "def net(channels=None, weight=0.125, pool=nn.MaxPool2d(2), extra_layers=(), res_layers=('layer1', 'layer3'), **kw):\n",
    "    channels = channels or {'prep': 64, 'layer1': 128, 'layer2': 256, 'layer3': 512}\n",
    "    residual = lambda c, **kw: {'in': Identity(), 'res1': conv_bn(c, c, **kw), 'res2': conv_bn(c, c, **kw), \n",
    "                                'add': (Add(), ['in', 'res2/relu'])}\n",
    "    n = basic_net(channels, weight, pool, **kw)\n",
    "    for layer in res_layers:\n",
    "        n[layer]['residual'] = residual(channels[layer], **kw)\n",
    "    for layer in extra_layers:\n",
    "        n[layer]['extra'] = conv_bn(channels[layer], channels[layer], **kw)       \n",
    "    return n\n",
    "\n",
    "remove_identity_nodes = lambda net: remove_by_type(net, Identity)\n",
    "\n",
    "def train(model, lr_schedule, train_set, test_set, batch_size, num_workers=0):\n",
    "    train_batches = DataLoader(train_set, batch_size, shuffle=True, set_random_choices=True, num_workers=num_workers)\n",
    "    test_batches = DataLoader(test_set, batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    lr = lambda step: lr_schedule(step/len(train_batches))/batch_size\n",
    "    opts = [SGD(trainable_params(model).values(), {'lr': lr, 'weight_decay': Const(5e-4*batch_size), 'momentum': Const(0.9)})]\n",
    "    logs, state = Table(), {MODEL: model, LOSS: x_ent_loss, OPTS: opts}\n",
    "    for epoch in range(lr_schedule.knots[-1]):\n",
    "        logs.append(union({'epoch': epoch+1, 'lr': lr_schedule(epoch+1)}, \n",
    "                          train_epoch(state, Timer(torch.cuda.synchronize), train_batches, test_batches)))\n",
    "    return logs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Preprocessing training data\n",
      "Finished in 1.6 seconds\n",
      "Preprocessing test data\n",
      "Finished in 0.066 seconds\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = '../../cifar-10-python/'\n",
    "dataset = cifar10(DATA_DIR)\n",
    "timer = Timer()\n",
    "print('Preprocessing training data')\n",
    "transforms = [\n",
    "    partial(normalise, mean=np.array(cifar10_mean, dtype=np.float32), std=np.array(cifar10_std, dtype=np.float32)),\n",
    "    partial(transpose, source='NHWC', target='NCHW'), \n",
    "]\n",
    "train_set = list(zip(*preprocess(dataset['train'], [partial(pad, border=4)] + transforms).values()))\n",
    "print(f'Finished in {timer():.2} seconds')\n",
    "print('Preprocessing test data')\n",
    "test_set = list(zip(*preprocess(dataset['valid'], transforms).values()))\n",
    "print(f'Finished in {timer():.2} seconds')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1x1conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pydot is needed for network visualisation"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "c:\\Users\\Administrator\\Desktop\\PRML\\PRML2023\\proj3\\torch_backend.py:243: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1025.)\n",
      "  dw.add_(weight_decay, w).mul_(-lr)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       epoch           lr   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
      "           1       0.1000       5.7514       2.1632       0.2573       0.2142       1.7764       0.3457       5.7514\n",
      "           2       0.2000       2.2766       1.7967       0.3425       0.1582       1.6905       0.4017       2.2766\n",
      "           3       0.3000       2.2493       1.6980       0.3822       0.1511       1.6319       0.4077       2.2493\n",
      "           4       0.4000       2.2360       1.6616       0.3977       0.1608       1.6039       0.4261       2.2360\n",
      "           5       0.3750       2.2701       1.6383       0.4064       0.1509       1.5561       0.4418       2.2701\n",
      "           6       0.3500       2.2868       1.6021       0.4220       0.1601       1.5527       0.4362       2.2868\n",
      "           7       0.3250       2.3301       1.5742       0.4303       0.1669       1.4972       0.4617       2.3301\n",
      "           8       0.3000       2.3133       1.5483       0.4395       0.1739       1.5520       0.4369       2.3133\n",
      "           9       0.2750       2.5138       1.5321       0.4486       0.1680       1.4856       0.4659       2.5138\n",
      "          10       0.2500       2.3112       1.5124       0.4520       0.1634       1.4276       0.4867       2.3112\n",
      "          11       0.2250       2.3544       1.4914       0.4611       0.1685       1.4433       0.4747       2.3544\n",
      "          12       0.2000       2.4190       1.4720       0.4728       0.1806       1.4287       0.4846       2.4190\n",
      "          13       0.1750       2.4047       1.4577       0.4749       0.1866       1.4522       0.4724       2.4047\n",
      "          14       0.1500       2.5211       1.4306       0.4873       0.1678       1.4082       0.4926       2.5211\n",
      "          15       0.1250       2.3632       1.4107       0.4945       0.1732       1.4147       0.4878       2.3632\n",
      "          16       0.1000       2.3414       1.3920       0.5023       0.1830       1.3402       0.5178       2.3414\n",
      "          17       0.0750       2.4798       1.3668       0.5099       0.1865       1.3198       0.5271       2.4798\n",
      "          18       0.0500       2.6118       1.3440       0.5191       0.1930       1.2796       0.5446       2.6118\n",
      "          19       0.0250       2.6848       1.3116       0.5300       0.1941       1.2638       0.5450       2.6848\n",
      "          20       0.0000       2.8939       1.2860       0.5413       0.1784       1.2259       0.5636       2.8939\n"
     ]
    }
   ],
   "source": [
    "def shortcut_block(c_in, c_out, stride, **kw):\n",
    "    projection = (stride != 1) or (c_in != c_out)\n",
    "    if projection:\n",
    "        return {\n",
    "            'conv':  nn.Conv2d(c_in, c_out, kernel_size=1, stride=stride, padding=0, bias=False), \n",
    "            'bn': batch_norm(c_out, **kw),\n",
    "            'relu': nn.ReLU(True),\n",
    "        }\n",
    "    else:\n",
    "        return {'id': Identity()}\n",
    "\n",
    "lr_schedule = PiecewiseLinear([0, 4, 20], [0, 0.4, 0])\n",
    "batch_size = 512\n",
    "\n",
    "n = DAWN_net(block=shortcut_block, prep_bn_relu=True)\n",
    "draw(build_graph(n))\n",
    "model = Network(n).to(device).half()\n",
    "train_set_x = Transform(train_set, [Crop(32, 32), FlipLR(), Cutout(8,8)])\n",
    "summary = train(model, lr_schedule, train_set_x, test_set, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3x3conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pydot is needed for network visualisation"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       epoch           lr   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
      "           1       0.1000       8.3853       1.9080       0.3319       0.4811       1.4173       0.4839       8.3853\n",
      "           2       0.2000       7.0179       1.4362       0.4807       0.3416       1.1469       0.5898       7.0179\n",
      "           3       0.3000       6.9860       1.2225       0.5640       0.3327       1.0714       0.6154       6.9860\n",
      "           4       0.4000       7.1769       1.1203       0.6029       0.3511       1.0311       0.6370       7.1769\n",
      "           5       0.3750       7.1263       1.0148       0.6419       0.3477       0.8888       0.6886       7.1263\n",
      "           6       0.3500       7.3021       0.9288       0.6728       0.3431       0.8877       0.6986       7.3021\n",
      "           7       0.3250       7.3242       0.8602       0.6974       0.3510       0.9942       0.6666       7.3242\n",
      "           8       0.3000       7.4000       0.8101       0.7158       0.3542       1.0178       0.6550       7.4000\n",
      "           9       0.2750       7.4386       0.7634       0.7334       0.3424       0.8009       0.7227       7.4386\n",
      "          10       0.2500       7.5072       0.7298       0.7428       0.3735       0.7482       0.7418       7.5072\n",
      "          11       0.2250       7.7167       0.6972       0.7565       0.3544       0.6598       0.7721       7.7167\n",
      "          12       0.2000       7.5076       0.6597       0.7714       0.3572       0.6638       0.7745       7.5076\n",
      "          13       0.1750       7.3900       0.6279       0.7810       0.3552       0.6133       0.7892       7.3900\n",
      "          14       0.1500       7.5039       0.5968       0.7913       0.3568       0.5454       0.8118       7.5039\n",
      "          15       0.1250       7.4592       0.5658       0.8033       0.3582       0.5804       0.7974       7.4592\n",
      "          16       0.1000       7.4625       0.5373       0.8127       0.3495       0.5178       0.8250       7.4625\n",
      "          17       0.0750       7.4201       0.5007       0.8253       0.3609       0.5043       0.8265       7.4201\n",
      "          18       0.0500       7.4694       0.4684       0.8365       0.3591       0.4473       0.8463       7.4694\n",
      "          19       0.0250       7.4221       0.4354       0.8499       0.3547       0.4549       0.8450       7.4221\n",
      "          20       0.0000       7.4575       0.4050       0.8586       0.3948       0.4113       0.8606       7.4575\n"
     ]
    }
   ],
   "source": [
    "def shortcut_block(c_in, c_out, stride, **kw):\n",
    "    projection = (stride != 1) or (c_in != c_out)\n",
    "    if projection:\n",
    "        return {\n",
    "            'conv': nn.Conv2d(c_in, c_out, kernel_size=3, stride=stride, padding=1, bias=False), \n",
    "            'bn': batch_norm(c_out, **kw),\n",
    "            'relu': nn.ReLU(True),\n",
    "        }\n",
    "    else:\n",
    "        return {'id': Identity()}\n",
    "\n",
    "lr_schedule = PiecewiseLinear([0, 4, 20], [0, 0.4, 0])\n",
    "batch_size = 512\n",
    "\n",
    "n = DAWN_net(block=shortcut_block, prep_bn_relu=True)\n",
    "draw(build_graph(n))\n",
    "model = Network(n).to(device).half()\n",
    "train_set_x = Transform(train_set, [Crop(32, 32), FlipLR(), Cutout(8,8)])\n",
    "summary = train(model, lr_schedule, train_set_x, test_set, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pydot is needed for network visualisation"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       epoch           lr   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
      "           1       0.1000      17.6184       2.6301       0.2590       1.2205       1.6399       0.4023      17.6184\n",
      "           2       0.2000      11.8325       1.5426       0.4441       0.7181       1.3402       0.5103      11.8325\n",
      "           3       0.3000      11.8162       1.2086       0.5693       0.7147       1.0695       0.6004      11.8162\n",
      "           4       0.4000      11.7407       1.0536       0.6328       0.7219       1.0388       0.6322      11.7407\n",
      "           5       0.3750      11.7504       0.8769       0.6948       0.7180       1.7411       0.5360      11.7504\n",
      "           6       0.3500      11.6977       0.8053       0.7229       0.7185       0.5967       0.7943      11.6977\n",
      "           7       0.3250      11.8204       0.6829       0.7624       0.7191       0.6444       0.7772      11.8204\n",
      "           8       0.3000      11.8723       0.6211       0.7840       0.7198       0.6829       0.7632      11.8723\n",
      "           9       0.2750      11.8972       0.5799       0.8015       0.7206       0.7017       0.7630      11.8972\n",
      "          10       0.2500      11.8582       0.5447       0.8129       0.7216       0.5871       0.7987      11.8582\n",
      "          11       0.2250      12.7451       0.5023       0.8239       0.7182       0.5387       0.8149      12.7451\n",
      "          12       0.2000      12.7133       0.4698       0.8378       0.7198       0.4611       0.8384      12.7133\n",
      "          13       0.1750      11.8719       0.4380       0.8484       0.7177       0.4359       0.8500      11.8719\n",
      "          14       0.1500      12.7079       0.3994       0.8620       0.7189       0.4554       0.8392      12.7079\n",
      "          15       0.1250      12.7207       0.3782       0.8696       1.6133       0.3931       0.8683      12.7207\n",
      "          16       0.1000      11.8456       0.3434       0.8810       0.7182       0.3960       0.8653      11.8456\n",
      "          17       0.0750      13.5862       0.3116       0.8932       0.7182       0.3966       0.8666      13.5862\n",
      "          18       0.0500      12.7086       0.2768       0.9053       1.6108       0.3317       0.8894      12.7086\n",
      "          19       0.0250      12.7590       0.2457       0.9167       0.7518       0.3006       0.8970      12.7590\n",
      "          20       0.0000      12.6947       0.2174       0.9272       0.7255       0.2787       0.9067      12.6947\n"
     ]
    }
   ],
   "source": [
    "def shortcut_block(c_in, c_out, stride, **kw):\n",
    "    projection = (stride != 1) or (c_in != c_out)\n",
    "    if projection:\n",
    "        return {\n",
    "            'conv': nn.Conv2d(c_in, c_out, kernel_size=3, stride=1, padding=1, bias=False), \n",
    "            'bn': batch_norm(c_out, **kw),\n",
    "            'relu': nn.ReLU(True),\n",
    "            'pool': nn.MaxPool2d(2),\n",
    "        }\n",
    "    else:\n",
    "        return {'id': Identity()}\n",
    "\n",
    "lr_schedule = PiecewiseLinear([0, 4, 20], [0, 0.4, 0])\n",
    "batch_size = 512\n",
    "\n",
    "n = DAWN_net(c=[64,128,256,512], block=shortcut_block, prep_bn_relu=True, concat_pool=False)\n",
    "draw(build_graph(n))\n",
    "model = Network(n).to(device).half()\n",
    "train_set_x = Transform(train_set, [Crop(32, 32), FlipLR(), Cutout(8,8)])\n",
    "summary = train(model, lr_schedule, train_set_x, test_set, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pydot is needed for network visualisation"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       epoch           lr   train time   train loss    train acc   valid time   valid loss    valid acc   total time\n",
      "           1       0.0800      18.7367       1.6489       0.4047       1.2715       1.2100       0.5499      18.7367\n",
      "           2       0.1600      16.6009       0.9494       0.6624       1.0272       1.0279       0.6544      16.6009\n",
      "           3       0.2400      16.6721       0.7362       0.7425       1.0589       1.1312       0.6476      16.6721\n",
      "           4       0.3200      17.2054       0.6191       0.7855       1.0985       0.6790       0.7642      17.2054\n",
      "           5       0.4000      17.4312       0.5612       0.8054       1.0870       0.6438       0.7775      17.4312\n",
      "           6       0.3789      17.2692       0.4995       0.8284       1.0920       0.4727       0.8389      17.2692\n",
      "           7       0.3579      17.2604       0.4427       0.8499       1.0917       0.6155       0.7845      17.2604\n",
      "           8       0.3368      17.1863       0.4127       0.8587       1.0923       0.5270       0.8198      17.1863\n",
      "           9       0.3158      17.1760       0.3817       0.8711       1.0915       0.4587       0.8435      17.1760\n",
      "          10       0.2947      17.1979       0.3629       0.8754       1.0897       0.4154       0.8595      17.1979\n",
      "          11       0.2737      17.1981       0.3385       0.8839       1.0883       0.5287       0.8290      17.1981\n",
      "          12       0.2526      17.4316       0.3298       0.8866       1.1236       0.4930       0.8403      17.4316\n",
      "          13       0.2316      17.5535       0.3064       0.8961       1.1336       0.3866       0.8699      17.5535\n",
      "          14       0.2105      19.2563       0.2906       0.9017       1.0871       0.4271       0.8573      19.2563\n",
      "          15       0.1895      18.9420       0.2733       0.9068       1.0624       0.4462       0.8558      18.9420\n",
      "          16       0.1684      19.1707       0.2489       0.9165       1.0982       0.3605       0.8788      19.1707\n",
      "          17       0.1474      19.9370       0.2302       0.9233       1.1051       0.3638       0.8786      19.9370\n",
      "          18       0.1263      20.0252       0.2089       0.9303       1.1168       0.3296       0.8885      20.0252\n",
      "          19       0.1053      19.3383       0.1881       0.9367       1.1073       0.3258       0.8916      19.3383\n",
      "          20       0.0842      20.1253       0.1642       0.9463       1.1209       0.2614       0.9136      20.1253\n",
      "          21       0.0632      19.9989       0.1409       0.9552       1.1081       0.2544       0.9155      19.9989\n",
      "          22       0.0421      19.1456       0.1165       0.9638       1.9866       0.2066       0.9319      19.1456\n",
      "          23       0.0211      20.0153       0.0924       0.9723       1.1083       0.1898       0.9371      20.0153\n",
      "          24       0.0000      20.0104       0.0772       0.9777       1.1045       0.1748       0.9430      20.0104\n"
     ]
    }
   ],
   "source": [
    "lr_schedule = PiecewiseLinear([0, 5, 24], [0, 0.4, 0])\n",
    "batch_size = 512\n",
    "\n",
    "n = net()\n",
    "draw(build_graph(n))\n",
    "model = Network(n).to(device).half()\n",
    "train_set_x = Transform(train_set, [Crop(32, 32), FlipLR(), Cutout(8,8)])\n",
    "summary = train(model, lr_schedule, train_set_x, test_set, batch_size=batch_size, num_workers=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
